# -*- coding: utf-8 -*-
"""Parking_Slot_Detection_Using_YOLO11_and_Grad_CAM(_XAI_).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l3mM6VKBDfMQ1Fpf6QbARjFIIsGBplK5

# **Parking Slot Detection Using YOLO11 and Grad-CAM( XAI )**
"""

from google.colab import drive
drive.mount('/content/drive')

"""## **Install Libraries**"""

# Install PyTorch, TorchVision, and TorchAudio with CUDA 11.8 support
!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 --index-url https://download.pytorch.org/whl/cu118

# Install Ultralytics and OpenCV
!pip install ultralytics opencv-python

# Upgrade Ultralytics (optional but safe to include)
!pip install --upgrade ultralytics

import torch, torchvision
print("Torch version:", torch.__version__)
print("Torch CUDA version:", torch.version.cuda)
print("TorchVision version:", torchvision.__version__)

!pip install numpy==1.24.4
!pip install roboflow

"""# **Import Dataset Using Roboflow (PkLot)**"""

from roboflow import Roboflow
rf = Roboflow(api_key="Your_API_Key")
project = rf.workspace("brad-dwyer").project("pklot-1tros")
version = project.version(4)
dataset = version.download("yolov11")

"""# **Traning Dataset Using Yolo11 Model**"""

!yolo train model=yolo11n.pt data="/content/PKLot-4/data.yaml" \
    epochs=10 imgsz=640 batch=32 \
    patience=7 cos_lr=True \
    hsv_h=0.02 hsv_s=0.7 hsv_v=0.4 \
    fliplr=0.5 scale=0.6  \
    degrees=5 translate=0.1  \
    optimizer=Adam

!pip install --upgrade numpy==1.23.5 opencv-python --force-reinstall

import numpy
import cv2
print("numpy version:", numpy.__version__)
print("cv2 version:", cv2.__version__)

"""# **Model Evaluaton**"""

# Load Trained Model Weights
from ultralytics import YOLO

# Load trained YOLO model
model = YOLO('/content/runs/detect/train/weights/best.pt')

# Evaluate model performance
metrics = model.val()  # Evaluates on validation dataset from data.yaml

# Mean Average Precision
print(f"mAP@0.5: {metrics.box.map50:.4f}")
print(f"mAP@0.5:0.95: {metrics.box.map:.4f}")

# Mean Precision and Recall (use as properties, NOT methods)
precision = metrics.box.mp
recall = metrics.box.mr
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")

# F1 Score
f1_score = 2 * (precision * recall) / (precision + recall + 1e-6)
print(f"F1 Score: {f1_score:.4f}")

import os
from PIL import Image
import matplotlib.pyplot as plt

# Path to the val3 folder
folder_path = '/content/runs/detect/val/'

# List all image files (you can filter for .png, .jpg, etc.)
image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

# Loop through and display each image
for img_file in image_files:
    img_path = os.path.join(folder_path, img_file)
    img = Image.open(img_path)

    plt.figure(figsize=(8, 8))
    plt.imshow(img)
    plt.title(img_file)
    plt.axis('off')
    plt.show()

"""# **Model Prediction**"""

from google.colab import files
from IPython.display import display
import matplotlib.pyplot as plt
from ultralytics import YOLO

# Step 1: Upload an image
uploaded = files.upload()
image_path = next(iter(uploaded))

# Step 2: Load the trained YOLO model
model = YOLO('/content/best (1).pt')  # Update the path if needed

# Step 3: Run inference on the uploaded image
results = model(image_path, conf=0.1)

# Step 4: Plot and display the result directly (no saving to disk)
rendered_img = results[0].plot()  # Returns a NumPy RGB image

# Step 5: Display using matplotlib
plt.figure(figsize=(10, 10))
plt.axis('off')
plt.imshow(rendered_img)
plt.show()

!pip install git+https://github.com/facebookresearch/segment-anything.git

!wget -P models/ https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
import torch

model_type = "vit_h"
sam_checkpoint = "/content/models/sam_vit_h_4b8939.pth"  # Update path to where you downloaded the file

# Load the SAM model
sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device="cuda" if torch.cuda.is_available() else "cpu")

"""# **SAM Mask Generation**"""

import cv2
import matplotlib.pyplot as plt
import numpy as np
from segment_anything import SamAutomaticMaskGenerator
from google.colab import files

# Upload image interactively
uploaded = files.upload()
image_path = next(iter(uploaded))  # Get uploaded image path

# Generate masks
mask_generator = SamAutomaticMaskGenerator(sam)
image_bgr = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
masks = mask_generator.generate(image_rgb)

# Visualize the image with masks
def show_masked_image(image, masks):
    masked_image = image.copy()
    for mask in masks:
        m = mask["segmentation"]
        color = np.random.randint(0, 255, (3,), dtype=np.uint8)
        masked_image[m] = masked_image[m] * 0.5 + color * 0.5
    return masked_image

result_image = show_masked_image(image_rgb, masks)

# Plot the result
plt.figure(figsize=(10, 10))
plt.imshow(result_image)
plt.axis("off")
plt.title("Image with SAM Masks")
plt.show()

"""# **Missparking Identification**"""

# Function to extract bounding boxes from SAM masks
def extract_boxes_from_masks(masks):
    boxes = []
    for mask in masks:
        segmentation = mask["segmentation"]
        y_indices, x_indices = np.where(segmentation)
        if len(x_indices) > 0 and len(y_indices) > 0:
            x_min, x_max = x_indices.min(), x_indices.max()
            y_min, y_max = y_indices.min(), y_indices.max()
            boxes.append([x_min, y_min, x_max, y_max])
    return boxes

# Get bounding boxes for all segmented regions
slot_boxes = extract_boxes_from_masks(masks)

# Function to calculate IoU
def calculate_iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    interArea = max(0, xB - xA) * max(0, yB - yA)
    boxAArea = max(1, (boxA[2] - boxA[0])) * max(1, (boxA[3] - boxA[1]))
    boxBArea = max(1, (boxB[2] - boxB[0])) * max(1, (boxB[3] - boxB[1]))
    return interArea / float(boxAArea + boxBArea - interArea + 1e-5)

# Detect misparked vehicles
misparked_vehicles = []
for i, vbox in enumerate(slot_boxes):
    count_overlap = 0
    for j, sbox in enumerate(slot_boxes):
        if i != j and calculate_iou(vbox, sbox) > 0.3:
            count_overlap += 1
    if count_overlap >= 2:
        misparked_vehicles.append(vbox)

# ðŸ”¢ Count of misparked vehicles
misparked_count = len(misparked_vehicles)
print(f"ðŸš« Number of misparked vehicles detected: {misparked_count}")

# âœ… Draw misparked vehicle boxes
output = image_rgb.copy()
for box in misparked_vehicles:
    cv2.rectangle(output, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)
    cv2.putText(output, "Misparked", (box[0], box[1] - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)

# ðŸ“· Show final image
plt.figure(figsize=(10, 10))
plt.imshow(output)
plt.axis("off")
plt.title(f"Misparked Vehicles: {misparked_count}")
plt.show()

"""# **Grad CAM Detection**"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
from ultralytics import YOLO
from google.colab import files

# Upload image and model
uploaded = files.upload()

# Get image and model paths
image_path = next((f for f in uploaded if f.lower().endswith(('jpg', 'jpeg', 'png'))), None)
model_path = next((f for f in uploaded if f.lower().endswith('.pt')), None)

# Load model
model = YOLO(model_path)

# Detect objects
results = model(image_path, conf=0.1)

# Load image
image = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Create a blank heatmap (same shape as image)
heatmap = np.zeros((image.shape[0], image.shape[1]), dtype=np.float32)

# Draw detections and build heatmap
for result in results:
    boxes = result.boxes
    for box in boxes:
        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
        conf = box.conf[0].cpu().numpy()
        cls = int(box.cls[0].cpu().numpy())
        label = model.names[cls]

        # Color code based on label
        color = (0, 255, 0) if label == 'space-empty' else (255, 0, 0)
        cv2.rectangle(image_rgb, (x1, y1), (x2, y2), color, 2)
        cv2.putText(image_rgb, f"{label} {conf:.2f}", (x1, y1-5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        # Increase heatmap intensity in the detected region by confidence
        heatmap[y1:y2, x1:x2] += conf

# Normalize heatmap to 0-255
heatmap = np.clip(heatmap / heatmap.max() * 255, 0, 255).astype(np.uint8)

# Apply colormap (JET: blue to red)
colored_heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
colored_heatmap = cv2.cvtColor(colored_heatmap, cv2.COLOR_BGR2RGB)

# Blend heatmap and original image
overlay = cv2.addWeighted(image_rgb, 0.7, colored_heatmap, 0.3, 0)

# Display results
plt.figure(figsize=(18, 8))

plt.subplot(1, 3, 1)
plt.title("YOLO Detections (Explainability)")
plt.imshow(image_rgb)
plt.axis('off')

plt.subplot(1, 3, 2)
plt.title("Detection Confidence Heatmap")
plt.imshow(colored_heatmap)
plt.axis('off')

plt.subplot(1, 3, 3)
plt.title("Overlay: Detection + Heatmap")
plt.imshow(overlay)
plt.axis('off')

plt.tight_layout()
plt.show()

"""# **Explanation for occupied slots**"""

# Install dependencies
!pip install ultralytics opencv-python matplotlib pandas

# Imports
import cv2
import numpy as np
from ultralytics import YOLO
import torch
import matplotlib.pyplot as plt
import pandas as pd
from google.colab import files

# Upload image and model file
uploaded = files.upload()
image_path = next((f for f in uploaded if f.lower().endswith(('jpg', 'jpeg', 'png'))), None)
model_path = next((f for f in uploaded if f.lower().endswith('.pt')), None)

# Load YOLO model
model = YOLO(model_path)

# Run detection
results = model(image_path, conf=0.25)
image = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Process detections â€” only occupied slots
occupied_slots = []
for result in results:
    for box, cls, conf in zip(result.boxes.xyxy, result.boxes.cls, result.boxes.conf):
        x1, y1, x2, y2 = map(int, box)
        class_name = model.names[int(cls)]
        if class_name.lower() == 'space-occupied':
            crop = image_rgb[y1:y2, x1:x2]

            # Saliency heatmap (edge-based explanation)
            gray_crop = cv2.cvtColor(crop, cv2.COLOR_RGB2GRAY)
            saliency = cv2.Laplacian(gray_crop, cv2.CV_64F)
            heatmap = np.absolute(saliency)

            # Robust normalization to 0-255 range
            heatmap_norm = cv2.normalize(heatmap, None, 0, 255, cv2.NORM_MINMAX)
            heatmap_uint8 = heatmap_norm.astype(np.uint8)

            # Apply a visually balanced colormap
            heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_TURBO)

            # Blend heatmap with original crop for better explainability
            blended = cv2.addWeighted(crop, 0.6, heatmap_color, 0.4, 0)

            # Explanation based on pixel edge density
            density = np.mean(heatmap_uint8)
            explanation = "High visual complexity â€” likely due to vehicle presence." if density > 20 else \
                          "Distinct edges and shapes â€” consistent with occupied state."

            # Draw result on original image
            cv2.rectangle(image_rgb, (x1, y1), (x2, y2), (255, 0, 0), 2)
            cv2.putText(image_rgb, f'Occupied {conf:.2f}', (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

            # Display visuals
            plt.figure(figsize=(16, 4))
            plt.subplot(1, 3, 1)
            plt.imshow(crop)
            plt.title("Slot Crop")

            plt.subplot(1, 3, 2)
            plt.imshow(blended)
            plt.title("Blended Heatmap")

            plt.subplot(1, 3, 3)
            plt.text(0, 0.5, explanation, fontsize=12, wrap=True)
            plt.axis('off')
            plt.title("Explanation")

            plt.tight_layout()
            plt.show()

            occupied_slots.append({
                'Box': (x1, y1, x2, y2),
                'Confidence': round(conf.item(), 3),
                'Density': round(density, 2),
                'Explanation': explanation
            })

# Show summary
df = pd.DataFrame(occupied_slots)
if not df.empty:
    print("\nDetected Occupied Slots:")
    print(df)
else:
    print("\nNo occupied slots detected.")

"""# **Explanation for Empty slots**"""

# Install dependencies
!pip install ultralytics matplotlib opencv-python

# Imports
import cv2
import numpy as np
from ultralytics import YOLO
import torch
import matplotlib.pyplot as plt
import pandas as pd
from google.colab import files

# Upload image and model
uploaded = files.upload()
image_path = next((f for f in uploaded if f.lower().endswith(('jpg', 'jpeg', 'png'))), None)
model_path = next((f for f in uploaded if f.lower().endswith('.pt')), None)

# Load YOLO model
model = YOLO(model_path)

# Run detection
results = model(image_path, conf=0.25)
image = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Process detections â€” get only empty slots
empty_slots = []
for result in results:
    for box, cls, conf in zip(result.boxes.xyxy, result.boxes.cls, result.boxes.conf):
        x1, y1, x2, y2 = map(int, box)
        class_name = model.names[int(cls)]
        if class_name.lower() == 'space-empty':
            crop = image_rgb[y1:y2, x1:x2]

            # Heatmap by saliency (simple for now â€” use Grad-CAM if needed)
            gray_crop = cv2.cvtColor(crop, cv2.COLOR_RGB2GRAY)
            saliency = cv2.Laplacian(gray_crop, cv2.CV_64F)
            heatmap = np.absolute(saliency)
            heatmap = (heatmap / heatmap.max() * 255).astype(np.uint8)
            heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

            # Explanation logic
            density = np.mean(heatmap)
            explanation = "Uniform low-detail region â€” indicates empty space." if density < 20 else "High variance; possible obstruction."

            # Draw result on image
            cv2.rectangle(image_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(image_rgb, f'Empty {conf:.2f}', (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

            # Show visuals
            plt.figure(figsize=(12, 4))
            plt.subplot(1, 3, 1)
            plt.imshow(crop)
            plt.title("Slot Crop")

            plt.subplot(1, 3, 2)
            plt.imshow(heatmap_color)
            plt.title("Heatmap")

            plt.subplot(1, 3, 3)
            plt.text(0, 0.5, explanation, fontsize=12, wrap=True)
            plt.axis('off')
            plt.title("Explanation")

            plt.tight_layout()
            plt.show()

            empty_slots.append({
                'Box': (x1, y1, x2, y2),
                'Confidence': round(conf.item(), 3),
                'Density': round(density, 2),
                'Explanation': explanation
            })

# Summary Table
df = pd.DataFrame(empty_slots)
if not df.empty:
    print("\nDetected Empty Slots:")
    print(df)
else:
    print("\nNo empty slots detected.")

"""# **Model Testing**

# **Load all Liberies**
"""

# Install PyTorch, TorchVision, and TorchAudio with CUDA 11.8 support
!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 --index-url https://download.pytorch.org/whl/cu118

# Install Ultralytics and OpenCV
!pip install ultralytics opencv-python
!pip install --upgrade ultralytics

!pip install numpy==1.23.5

!pip install -q segment-anything ultralytics

!mkdir -p models
!wget -q -P models/ https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

import torch, torchvision
print("Torch version:", torch.__version__)
print("Torch CUDA version:", torch.version.cuda)
print("TorchVision version:", torchvision.__version__)

from google.colab import files
import os, torch, cv2, numpy as np
from PIL import Image
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ultralytics import YOLO

"""# **Output 1**"""

# ===============================
# ðŸ“¦ Setup + Upload Everything
# ===============================
uploaded = files.upload()  # Upload all files once (images + .pt model)
uploaded_files = list(uploaded.keys())

image_path = next((f for f in uploaded if f.lower().endswith(('jpg', 'jpeg', 'png'))), None)
model_path = next((f for f in uploaded_files if f.lower().endswith('.pt')), None)
assert model_path, "âŒ YOLO model file (.pt) not found!"

# ===============================
# ðŸ“· 1. YOLO Validation Visuals
# ===============================

model = YOLO(model_path)

# Step 3: Run inference on the uploaded image
results = model(image_path, conf=0.1)

# Step 4: Plot and display the result directly (no saving to disk)
rendered_img = results[0].plot()  # Returns a NumPy RGB image

# Step 5: Display using matplotlib
print("ðŸ” Our Model detection: ")
plt.figure(figsize=(10, 10))
plt.axis('off')
plt.imshow(rendered_img)
plt.show()

# ===============================
# ðŸ§  2. Segment Anything (SAM)
# ===============================
print("ðŸ“¥ Loading SAM model...")
sam = sam_model_registry["vit_h"](checkpoint="/content/models/sam_vit_h_4b8939.pth")
device = "cuda" if torch.cuda.is_available() else "cpu"
sam.to(device)

# Generate masks
mask_generator = SamAutomaticMaskGenerator(sam)
image_bgr = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
masks = mask_generator.generate(image_rgb)

# Visualize the image with masks
def show_masked_image(image, masks):
    masked_image = image.copy()
    for mask in masks:
        m = mask["segmentation"]
        color = np.random.randint(0, 255, (3,), dtype=np.uint8)
        masked_image[m] = masked_image[m] * 0.5 + color * 0.5
    return masked_image

result_image = show_masked_image(image_rgb, masks)

# Plot the result
plt.figure(figsize=(10, 10))
plt.imshow(result_image)
plt.axis("off")
plt.title("Image with SAM Segmentation")
plt.show()

# ===============================
# ðŸš˜ 3. Misparking Detection
# ===============================
# Function to extract bounding boxes from SAM masks
def extract_boxes_from_masks(masks):
    boxes = []
    for mask in masks:
        segmentation = mask["segmentation"]
        y_indices, x_indices = np.where(segmentation)
        if len(x_indices) > 0 and len(y_indices) > 0:
            x_min, x_max = x_indices.min(), x_indices.max()
            y_min, y_max = y_indices.min(), y_indices.max()
            boxes.append([x_min, y_min, x_max, y_max])
    return boxes

# Get bounding boxes for all segmented regions
slot_boxes = extract_boxes_from_masks(masks)

# Function to calculate IoU
def calculate_iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    interArea = max(0, xB - xA) * max(0, yB - yA)
    boxAArea = max(1, (boxA[2] - boxA[0])) * max(1, (boxA[3] - boxA[1]))
    boxBArea = max(1, (boxB[2] - boxB[0])) * max(1, (boxB[3] - boxB[1]))
    return interArea / float(boxAArea + boxBArea - interArea + 1e-5)

# Detect misparked vehicles
misparked_vehicles = []
for i, vbox in enumerate(slot_boxes):
    count_overlap = 0
    for j, sbox in enumerate(slot_boxes):
        if i != j and calculate_iou(vbox, sbox) > 0.3:
            count_overlap += 1
    if count_overlap >= 2:
        misparked_vehicles.append(vbox)

# ðŸ”¢ Count of misparked vehicles
misparked_count = len(misparked_vehicles)
print(f"ðŸš« Number of misparked vehicles detected: {misparked_count}")

# âœ… Draw misparked vehicle boxes
output = image_rgb.copy()
for box in misparked_vehicles:
    cv2.rectangle(output, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)
    cv2.putText(output, "Misparked", (box[0], box[1] - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)

# ðŸ“· Show final image
plt.figure(figsize=(10, 10))
plt.imshow(output)
plt.axis("off")
plt.title(f"Misparked Vehicles: {misparked_count}")
plt.show()

# ===============================
# ðŸ”¥ 4. Explainable Detection + Heatmap
# ===============================
print("ðŸ§  Explainable Detection on uploaded image...")
model = YOLO(model_path)
results = model(image_path, conf=0.1)
heatmap = np.zeros((image_rgb.shape[0], image_rgb.shape[1]), dtype=np.float32)

# Draw detections and build heatmap
for result in results:
    boxes = result.boxes
    for box in boxes:
        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
        conf = box.conf[0].cpu().numpy()
        cls = int(box.cls[0].cpu().numpy())
        label = model.names[cls]

        # Color code based on label
        color = (0, 255, 0) if label == 'space-empty' else (255, 0, 0)
        cv2.rectangle(image_rgb, (x1, y1), (x2, y2), color, 2)
        cv2.putText(image_rgb, f"{label} {conf:.2f}", (x1, y1-5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        # Increase heatmap intensity in the detected region by confidence
        heatmap[y1:y2, x1:x2] += conf

# Normalize heatmap to 0-255
heatmap = np.clip(heatmap / heatmap.max() * 255, 0, 255).astype(np.uint8)

# Apply colormap (JET: blue to red)
colored_heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
colored_heatmap = cv2.cvtColor(colored_heatmap, cv2.COLOR_BGR2RGB)

# Blend heatmap and original image
overlay = cv2.addWeighted(image_rgb, 0.7, colored_heatmap, 0.3, 0)

# Display results
plt.figure(figsize=(18, 8))

plt.subplot(1, 3, 1)
plt.title("YOLO Detections (Explainability)")
plt.imshow(image_rgb)
plt.axis('off')

plt.subplot(1, 3, 2)
plt.title("Detection Confidence Heatmap")
plt.imshow(colored_heatmap)
plt.axis('off')

plt.subplot(1, 3, 3)
plt.title("Overlay: Detection + Heatmap")
plt.imshow(overlay)
plt.axis('off')

plt.tight_layout()
plt.show()

# ===============================
# ðŸ”¥ 5. occupied slots Explainable Detection + Heatmap
# ===============================

# Load YOLO model
model = YOLO(model_path)

# Run detection
results = model(image_path, conf=0.25)
image = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Process detections â€” only occupied slots
occupied_slots = []
for result in results:
    for box, cls, conf in zip(result.boxes.xyxy, result.boxes.cls, result.boxes.conf):
        x1, y1, x2, y2 = map(int, box)
        class_name = model.names[int(cls)]
        if class_name.lower() == 'space-occupied':
            crop = image_rgb[y1:y2, x1:x2]

            # Saliency heatmap (edge-based explanation)
            gray_crop = cv2.cvtColor(crop, cv2.COLOR_RGB2GRAY)
            saliency = cv2.Laplacian(gray_crop, cv2.CV_64F)
            heatmap = np.absolute(saliency)

            # Robust normalization to 0-255 range
            heatmap_norm = cv2.normalize(heatmap, None, 0, 255, cv2.NORM_MINMAX)
            heatmap_uint8 = heatmap_norm.astype(np.uint8)

            # Apply a visually balanced colormap
            heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_TURBO)

            # Blend heatmap with original crop for better explainability
            blended = cv2.addWeighted(crop, 0.6, heatmap_color, 0.4, 0)

            # Explanation based on pixel edge density
            density = np.mean(heatmap_uint8)
            explanation = "High visual complexity â€” likely due to vehicle presence." if density > 20 else \
                          "Distinct edges and shapes â€” consistent with occupied state."

            # Draw result on original image
            cv2.rectangle(image_rgb, (x1, y1), (x2, y2), (255, 0, 0), 2)
            cv2.putText(image_rgb, f'Occupied {conf:.2f}', (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

            # Display visuals
            plt.figure(figsize=(16, 4))
            plt.subplot(1, 3, 1)
            plt.imshow(crop)
            plt.title("Slot Crop")

            plt.subplot(1, 3, 2)
            plt.imshow(blended)
            plt.title("Blended Heatmap")

            plt.subplot(1, 3, 3)
            plt.text(0, 0.5, explanation, fontsize=12, wrap=True)
            plt.axis('off')
            plt.title("Explanation")

            plt.tight_layout()
            plt.show()

            occupied_slots.append({
                'Box': (x1, y1, x2, y2),
                'Confidence': round(conf.item(), 3),
                'Density': round(density, 2),
                'Explanation': explanation
            })

# Show summary
df = pd.DataFrame(occupied_slots)
if not df.empty:
    print("\nDetected Occupied Slots:")
    print(df)
else:
    print("\nNo occupied slots detected.")


# ===============================
# ðŸ”¥ 6. non-occupied slots Explainable Detection + Heatmap
# ===============================

# Load YOLO model
model = YOLO(model_path)

# Run detection
results = model(image_path, conf=0.25)
image = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Process detections â€” get only empty slots
empty_slots = []
for result in results:
    for box, cls, conf in zip(result.boxes.xyxy, result.boxes.cls, result.boxes.conf):
        x1, y1, x2, y2 = map(int, box)
        class_name = model.names[int(cls)]
        if class_name.lower() == 'space-empty':
            crop = image_rgb[y1:y2, x1:x2]

            # Heatmap by saliency (simple for now â€” use Grad-CAM if needed)
            gray_crop = cv2.cvtColor(crop, cv2.COLOR_RGB2GRAY)
            saliency = cv2.Laplacian(gray_crop, cv2.CV_64F)
            heatmap = np.absolute(saliency)
            heatmap = (heatmap / heatmap.max() * 255).astype(np.uint8)
            heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

            # Explanation logic
            density = np.mean(heatmap)
            explanation = "Uniform low-detail region â€” indicates empty space." if density < 20 else "High variance; possible obstruction."

            # Draw result on image
            cv2.rectangle(image_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(image_rgb, f'Empty {conf:.2f}', (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

            # Show visuals
            plt.figure(figsize=(12, 4))
            plt.subplot(1, 3, 1)
            plt.imshow(crop)
            plt.title("Slot Crop")

            plt.subplot(1, 3, 2)
            plt.imshow(heatmap_color)
            plt.title("Heatmap")

            plt.subplot(1, 3, 3)
            plt.text(0, 0.5, explanation, fontsize=12, wrap=True)
            plt.axis('off')
            plt.title("Explanation")

            plt.tight_layout()
            plt.show()

            empty_slots.append({
                'Box': (x1, y1, x2, y2),
                'Confidence': round(conf.item(), 3),
                'Density': round(density, 2),
                'Explanation': explanation
            })

# Summary Table
df = pd.DataFrame(empty_slots)
if not df.empty:
    print("\nDetected Empty Slots:")
    print(df)
else:
    print("\nNo empty slots detected.")

"""# **Output 2**"""

# ===============================
# ðŸ“¦ Setup + Upload Everything
# ===============================
uploaded = files.upload()  # Upload all files once (images + .pt model)
uploaded_files = list(uploaded.keys())

image_path = next((f for f in uploaded if f.lower().endswith(('jpg', 'jpeg', 'png'))), None)
model_path = next((f for f in uploaded_files if f.lower().endswith('.pt')), None)
assert model_path, "âŒ YOLO model file (.pt) not found!"

# ===============================
# ðŸ“· 1. YOLO Validation Visuals
# ===============================

model = YOLO(model_path)

# Step 3: Run inference on the uploaded image
results = model(image_path, conf=0.1)

# Step 4: Plot and display the result directly (no saving to disk)
rendered_img = results[0].plot()  # Returns a NumPy RGB image

# Step 5: Display using matplotlib
print("ðŸ” Our Model detection: ")
plt.figure(figsize=(10, 10))
plt.axis('off')
plt.imshow(rendered_img)
plt.show()

# ===============================
# ðŸ§  2. Segment Anything (SAM)
# ===============================
print("ðŸ“¥ Loading SAM model...")
sam = sam_model_registry["vit_h"](checkpoint="/content/models/sam_vit_h_4b8939.pth")
device = "cuda" if torch.cuda.is_available() else "cpu"
sam.to(device)

# Generate masks
mask_generator = SamAutomaticMaskGenerator(sam)
image_bgr = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
masks = mask_generator.generate(image_rgb)

# Visualize the image with masks
def show_masked_image(image, masks):
    masked_image = image.copy()
    for mask in masks:
        m = mask["segmentation"]
        color = np.random.randint(0, 255, (3,), dtype=np.uint8)
        masked_image[m] = masked_image[m] * 0.5 + color * 0.5
    return masked_image

result_image = show_masked_image(image_rgb, masks)

# Plot the result
plt.figure(figsize=(10, 10))
plt.imshow(result_image)
plt.axis("off")
plt.title("Image with SAM Segmentation")
plt.show()

# ===============================
# ðŸš˜ 3. Misparking Detection
# ===============================
# Function to extract bounding boxes from SAM masks
def extract_boxes_from_masks(masks):
    boxes = []
    for mask in masks:
        segmentation = mask["segmentation"]
        y_indices, x_indices = np.where(segmentation)
        if len(x_indices) > 0 and len(y_indices) > 0:
            x_min, x_max = x_indices.min(), x_indices.max()
            y_min, y_max = y_indices.min(), y_indices.max()
            boxes.append([x_min, y_min, x_max, y_max])
    return boxes

# Get bounding boxes for all segmented regions
slot_boxes = extract_boxes_from_masks(masks)

# Function to calculate IoU
def calculate_iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    interArea = max(0, xB - xA) * max(0, yB - yA)
    boxAArea = max(1, (boxA[2] - boxA[0])) * max(1, (boxA[3] - boxA[1]))
    boxBArea = max(1, (boxB[2] - boxB[0])) * max(1, (boxB[3] - boxB[1]))
    return interArea / float(boxAArea + boxBArea - interArea + 1e-5)

# Detect misparked vehicles
misparked_vehicles = []
for i, vbox in enumerate(slot_boxes):
    count_overlap = 0
    for j, sbox in enumerate(slot_boxes):
        if i != j and calculate_iou(vbox, sbox) > 0.3:
            count_overlap += 1
    if count_overlap >= 2:
        misparked_vehicles.append(vbox)

# ðŸ”¢ Count of misparked vehicles
misparked_count = len(misparked_vehicles)
print(f"ðŸš« Number of misparked vehicles detected: {misparked_count}")

# âœ… Draw misparked vehicle boxes
output = image_rgb.copy()
for box in misparked_vehicles:
    cv2.rectangle(output, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)
    cv2.putText(output, "Misparked", (box[0], box[1] - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)

# ðŸ“· Show final image
plt.figure(figsize=(10, 10))
plt.imshow(output)
plt.axis("off")
plt.title(f"Misparked Vehicles: {misparked_count}")
plt.show()

# ===============================
# ðŸ”¥ 4. Explainable Detection + Heatmap
# ===============================
print("ðŸ§  Explainable Detection on uploaded image...")
model = YOLO(model_path)
results = model(image_path, conf=0.1)
heatmap = np.zeros((image_rgb.shape[0], image_rgb.shape[1]), dtype=np.float32)

# Draw detections and build heatmap
for result in results:
    boxes = result.boxes
    for box in boxes:
        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
        conf = box.conf[0].cpu().numpy()
        cls = int(box.cls[0].cpu().numpy())
        label = model.names[cls]

        # Color code based on label
        color = (0, 255, 0) if label == 'space-empty' else (255, 0, 0)
        cv2.rectangle(image_rgb, (x1, y1), (x2, y2), color, 2)
        cv2.putText(image_rgb, f"{label} {conf:.2f}", (x1, y1-5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        # Increase heatmap intensity in the detected region by confidence
        heatmap[y1:y2, x1:x2] += conf

# Normalize heatmap to 0-255
heatmap = np.clip(heatmap / heatmap.max() * 255, 0, 255).astype(np.uint8)

# Apply colormap (JET: blue to red)
colored_heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
colored_heatmap = cv2.cvtColor(colored_heatmap, cv2.COLOR_BGR2RGB)

# Blend heatmap and original image
overlay = cv2.addWeighted(image_rgb, 0.7, colored_heatmap, 0.3, 0)

# Display results
plt.figure(figsize=(18, 8))

plt.subplot(1, 3, 1)
plt.title("YOLO Detections (Explainability)")
plt.imshow(image_rgb)
plt.axis('off')

plt.subplot(1, 3, 2)
plt.title("Detection Confidence Heatmap")
plt.imshow(colored_heatmap)
plt.axis('off')

plt.subplot(1, 3, 3)
plt.title("Overlay: Detection + Heatmap")
plt.imshow(overlay)
plt.axis('off')

plt.tight_layout()
plt.show()

# ===============================
# ðŸ”¥ 5. occupied slots Explainable Detection + Heatmap
# ===============================

# Load YOLO model
model = YOLO(model_path)

# Run detection
results = model(image_path, conf=0.25)
image = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Process detections â€” only occupied slots
occupied_slots = []
for result in results:
    for box, cls, conf in zip(result.boxes.xyxy, result.boxes.cls, result.boxes.conf):
        x1, y1, x2, y2 = map(int, box)
        class_name = model.names[int(cls)]
        if class_name.lower() == 'space-occupied':
            crop = image_rgb[y1:y2, x1:x2]

            # Saliency heatmap (edge-based explanation)
            gray_crop = cv2.cvtColor(crop, cv2.COLOR_RGB2GRAY)
            saliency = cv2.Laplacian(gray_crop, cv2.CV_64F)
            heatmap = np.absolute(saliency)

            # Robust normalization to 0-255 range
            heatmap_norm = cv2.normalize(heatmap, None, 0, 255, cv2.NORM_MINMAX)
            heatmap_uint8 = heatmap_norm.astype(np.uint8)

            # Apply a visually balanced colormap
            heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_TURBO)

            # Blend heatmap with original crop for better explainability
            blended = cv2.addWeighted(crop, 0.6, heatmap_color, 0.4, 0)

            # Explanation based on pixel edge density
            density = np.mean(heatmap_uint8)
            explanation = "High visual complexity â€” likely due to vehicle presence." if density > 20 else \
                          "Distinct edges and shapes â€” consistent with occupied state."

            # Draw result on original image
            cv2.rectangle(image_rgb, (x1, y1), (x2, y2), (255, 0, 0), 2)
            cv2.putText(image_rgb, f'Occupied {conf:.2f}', (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

            # Display visuals
            plt.figure(figsize=(16, 4))
            plt.subplot(1, 3, 1)
            plt.imshow(crop)
            plt.title("Slot Crop")

            plt.subplot(1, 3, 2)
            plt.imshow(blended)
            plt.title("Blended Heatmap")

            plt.subplot(1, 3, 3)
            plt.text(0, 0.5, explanation, fontsize=12, wrap=True)
            plt.axis('off')
            plt.title("Explanation")

            plt.tight_layout()
            plt.show()

            occupied_slots.append({
                'Box': (x1, y1, x2, y2),
                'Confidence': round(conf.item(), 3),
                'Density': round(density, 2),
                'Explanation': explanation
            })

# Show summary
df = pd.DataFrame(occupied_slots)
if not df.empty:
    print("\nDetected Occupied Slots:")
    print(df)
else:
    print("\nNo occupied slots detected.")


# ===============================
# ðŸ”¥ 6. non-occupied slots Explainable Detection + Heatmap
# ===============================

# Load YOLO model
model = YOLO(model_path)

# Run detection
results = model(image_path, conf=0.25)
image = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Process detections â€” get only empty slots
empty_slots = []
for result in results:
    for box, cls, conf in zip(result.boxes.xyxy, result.boxes.cls, result.boxes.conf):
        x1, y1, x2, y2 = map(int, box)
        class_name = model.names[int(cls)]
        if class_name.lower() == 'space-empty':
            crop = image_rgb[y1:y2, x1:x2]

            # Heatmap by saliency (simple for now â€” use Grad-CAM if needed)
            gray_crop = cv2.cvtColor(crop, cv2.COLOR_RGB2GRAY)
            saliency = cv2.Laplacian(gray_crop, cv2.CV_64F)
            heatmap = np.absolute(saliency)
            heatmap = (heatmap / heatmap.max() * 255).astype(np.uint8)
            heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

            # Explanation logic
            density = np.mean(heatmap)
            explanation = "Uniform low-detail region â€” indicates empty space." if density < 20 else "High variance; possible obstruction."

            # Draw result on image
            cv2.rectangle(image_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(image_rgb, f'Empty {conf:.2f}', (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

            # Show visuals
            plt.figure(figsize=(12, 4))
            plt.subplot(1, 3, 1)
            plt.imshow(crop)
            plt.title("Slot Crop")

            plt.subplot(1, 3, 2)
            plt.imshow(heatmap_color)
            plt.title("Heatmap")

            plt.subplot(1, 3, 3)
            plt.text(0, 0.5, explanation, fontsize=12, wrap=True)
            plt.axis('off')
            plt.title("Explanation")

            plt.tight_layout()
            plt.show()

            empty_slots.append({
                'Box': (x1, y1, x2, y2),
                'Confidence': round(conf.item(), 3),
                'Density': round(density, 2),
                'Explanation': explanation
            })

# Summary Table
df = pd.DataFrame(empty_slots)
if not df.empty:
    print("\nDetected Empty Slots:")
    print(df)
else:
    print("\nNo empty slots detected.")

"""# **Output 3**"""

# ===============================
# ðŸ“¦ Setup + Upload Everything
# ===============================
uploaded = files.upload()  # Upload all files once (images + .pt model)
uploaded_files = list(uploaded.keys())

image_path = next((f for f in uploaded if f.lower().endswith(('jpg', 'jpeg', 'png'))), None)
model_path = next((f for f in uploaded_files if f.lower().endswith('.pt')), None)
assert model_path, "âŒ YOLO model file (.pt) not found!"

# ===============================
# ðŸ“· 1. YOLO Validation Visuals
# ===============================

model = YOLO(model_path)

# Step 3: Run inference on the uploaded image
results = model(image_path, conf=0.1)

# Step 4: Plot and display the result directly (no saving to disk)
rendered_img = results[0].plot()  # Returns a NumPy RGB image

# Step 5: Display using matplotlib
print("ðŸ” Our Model detection: ")
plt.figure(figsize=(10, 10))
plt.axis('off')
plt.imshow(rendered_img)
plt.show()

# ===============================
# ðŸ§  2. Segment Anything (SAM)
# ===============================
print("ðŸ“¥ Loading SAM model...")
sam = sam_model_registry["vit_h"](checkpoint="/content/models/sam_vit_h_4b8939.pth")
device = "cuda" if torch.cuda.is_available() else "cpu"
sam.to(device)

# Generate masks
mask_generator = SamAutomaticMaskGenerator(sam)
image_bgr = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
masks = mask_generator.generate(image_rgb)

# Visualize the image with masks
def show_masked_image(image, masks):
    masked_image = image.copy()
    for mask in masks:
        m = mask["segmentation"]
        color = np.random.randint(0, 255, (3,), dtype=np.uint8)
        masked_image[m] = masked_image[m] * 0.5 + color * 0.5
    return masked_image

result_image = show_masked_image(image_rgb, masks)

# Plot the result
plt.figure(figsize=(10, 10))
plt.imshow(result_image)
plt.axis("off")
plt.title("Image with SAM Segmentation")
plt.show()

# ===============================
# ðŸš˜ 3. Misparking Detection
# ===============================
# Function to extract bounding boxes from SAM masks
def extract_boxes_from_masks(masks):
    boxes = []
    for mask in masks:
        segmentation = mask["segmentation"]
        y_indices, x_indices = np.where(segmentation)
        if len(x_indices) > 0 and len(y_indices) > 0:
            x_min, x_max = x_indices.min(), x_indices.max()
            y_min, y_max = y_indices.min(), y_indices.max()
            boxes.append([x_min, y_min, x_max, y_max])
    return boxes

# Get bounding boxes for all segmented regions
slot_boxes = extract_boxes_from_masks(masks)

# Function to calculate IoU
def calculate_iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    interArea = max(0, xB - xA) * max(0, yB - yA)
    boxAArea = max(1, (boxA[2] - boxA[0])) * max(1, (boxA[3] - boxA[1]))
    boxBArea = max(1, (boxB[2] - boxB[0])) * max(1, (boxB[3] - boxB[1]))
    return interArea / float(boxAArea + boxBArea - interArea + 1e-5)

# Detect misparked vehicles
misparked_vehicles = []
for i, vbox in enumerate(slot_boxes):
    count_overlap = 0
    for j, sbox in enumerate(slot_boxes):
        if i != j and calculate_iou(vbox, sbox) > 0.3:
            count_overlap += 1
    if count_overlap >= 2:
        misparked_vehicles.append(vbox)

# ðŸ”¢ Count of misparked vehicles
misparked_count = len(misparked_vehicles)
print(f"ðŸš« Number of misparked vehicles detected: {misparked_count}")

# âœ… Draw misparked vehicle boxes
output = image_rgb.copy()
for box in misparked_vehicles:
    cv2.rectangle(output, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)
    cv2.putText(output, "Misparked", (box[0], box[1] - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)

# ðŸ“· Show final image
plt.figure(figsize=(10, 10))
plt.imshow(output)
plt.axis("off")
plt.title(f"Misparked Vehicles: {misparked_count}")
plt.show()

# ===============================
# ðŸ”¥ 4. Explainable Detection + Heatmap
# ===============================
print("ðŸ§  Explainable Detection on uploaded image...")
model = YOLO(model_path)
results = model(image_path, conf=0.1)
heatmap = np.zeros((image_rgb.shape[0], image_rgb.shape[1]), dtype=np.float32)

# Draw detections and build heatmap
for result in results:
    boxes = result.boxes
    for box in boxes:
        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
        conf = box.conf[0].cpu().numpy()
        cls = int(box.cls[0].cpu().numpy())
        label = model.names[cls]

        # Color code based on label
        color = (0, 255, 0) if label == 'space-empty' else (255, 0, 0)
        cv2.rectangle(image_rgb, (x1, y1), (x2, y2), color, 2)
        cv2.putText(image_rgb, f"{label} {conf:.2f}", (x1, y1-5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        # Increase heatmap intensity in the detected region by confidence
        heatmap[y1:y2, x1:x2] += conf

# Normalize heatmap to 0-255
heatmap = np.clip(heatmap / heatmap.max() * 255, 0, 255).astype(np.uint8)

# Apply colormap (JET: blue to red)
colored_heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
colored_heatmap = cv2.cvtColor(colored_heatmap, cv2.COLOR_BGR2RGB)

# Blend heatmap and original image
overlay = cv2.addWeighted(image_rgb, 0.7, colored_heatmap, 0.3, 0)

# Display results
plt.figure(figsize=(18, 8))

plt.subplot(1, 3, 1)
plt.title("YOLO Detections (Explainability)")
plt.imshow(image_rgb)
plt.axis('off')

plt.subplot(1, 3, 2)
plt.title("Detection Confidence Heatmap")
plt.imshow(colored_heatmap)
plt.axis('off')

plt.subplot(1, 3, 3)
plt.title("Overlay: Detection + Heatmap")
plt.imshow(overlay)
plt.axis('off')

plt.tight_layout()
plt.show()

# ===============================
# ðŸ”¥ 5. occupied slots Explainable Detection + Heatmap
# ===============================

# Load YOLO model
model = YOLO(model_path)

# Run detection
results = model(image_path, conf=0.25)
image = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Process detections â€” only occupied slots
occupied_slots = []
for result in results:
    for box, cls, conf in zip(result.boxes.xyxy, result.boxes.cls, result.boxes.conf):
        x1, y1, x2, y2 = map(int, box)
        class_name = model.names[int(cls)]
        if class_name.lower() == 'space-occupied':
            crop = image_rgb[y1:y2, x1:x2]

            # Saliency heatmap (edge-based explanation)
            gray_crop = cv2.cvtColor(crop, cv2.COLOR_RGB2GRAY)
            saliency = cv2.Laplacian(gray_crop, cv2.CV_64F)
            heatmap = np.absolute(saliency)

            # Robust normalization to 0-255 range
            heatmap_norm = cv2.normalize(heatmap, None, 0, 255, cv2.NORM_MINMAX)
            heatmap_uint8 = heatmap_norm.astype(np.uint8)

            # Apply a visually balanced colormap
            heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_TURBO)

            # Blend heatmap with original crop for better explainability
            blended = cv2.addWeighted(crop, 0.6, heatmap_color, 0.4, 0)

            # Explanation based on pixel edge density
            density = np.mean(heatmap_uint8)
            explanation = "High visual complexity â€” likely due to vehicle presence." if density > 20 else \
                          "Distinct edges and shapes â€” consistent with occupied state."

            # Draw result on original image
            cv2.rectangle(image_rgb, (x1, y1), (x2, y2), (255, 0, 0), 2)
            cv2.putText(image_rgb, f'Occupied {conf:.2f}', (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

            # Display visuals
            plt.figure(figsize=(16, 4))
            plt.subplot(1, 3, 1)
            plt.imshow(crop)
            plt.title("Slot Crop")

            plt.subplot(1, 3, 2)
            plt.imshow(blended)
            plt.title("Blended Heatmap")

            plt.subplot(1, 3, 3)
            plt.text(0, 0.5, explanation, fontsize=12, wrap=True)
            plt.axis('off')
            plt.title("Explanation")

            plt.tight_layout()
            plt.show()

            occupied_slots.append({
                'Box': (x1, y1, x2, y2),
                'Confidence': round(conf.item(), 3),
                'Density': round(density, 2),
                'Explanation': explanation
            })

# Show summary
df = pd.DataFrame(occupied_slots)
if not df.empty:
    print("\nDetected Occupied Slots:")
    print(df)
else:
    print("\nNo occupied slots detected.")


# ===============================
# ðŸ”¥ 6. non-occupied slots Explainable Detection + Heatmap
# ===============================

# Load YOLO model
model = YOLO(model_path)

# Run detection
results = model(image_path, conf=0.25)
image = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Process detections â€” get only empty slots
empty_slots = []
for result in results:
    for box, cls, conf in zip(result.boxes.xyxy, result.boxes.cls, result.boxes.conf):
        x1, y1, x2, y2 = map(int, box)
        class_name = model.names[int(cls)]
        if class_name.lower() == 'space-empty':
            crop = image_rgb[y1:y2, x1:x2]

            # Heatmap by saliency (simple for now â€” use Grad-CAM if needed)
            gray_crop = cv2.cvtColor(crop, cv2.COLOR_RGB2GRAY)
            saliency = cv2.Laplacian(gray_crop, cv2.CV_64F)
            heatmap = np.absolute(saliency)
            heatmap = (heatmap / heatmap.max() * 255).astype(np.uint8)
            heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

            # Explanation logic
            density = np.mean(heatmap)
            explanation = "Uniform low-detail region â€” indicates empty space." if density < 20 else "High variance; possible obstruction."

            # Draw result on image
            cv2.rectangle(image_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(image_rgb, f'Empty {conf:.2f}', (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

            # Show visuals
            plt.figure(figsize=(12, 4))
            plt.subplot(1, 3, 1)
            plt.imshow(crop)
            plt.title("Slot Crop")

            plt.subplot(1, 3, 2)
            plt.imshow(heatmap_color)
            plt.title("Heatmap")

            plt.subplot(1, 3, 3)
            plt.text(0, 0.5, explanation, fontsize=12, wrap=True)
            plt.axis('off')
            plt.title("Explanation")

            plt.tight_layout()
            plt.show()

            empty_slots.append({
                'Box': (x1, y1, x2, y2),
                'Confidence': round(conf.item(), 3),
                'Density': round(density, 2),
                'Explanation': explanation
            })

# Summary Table
df = pd.DataFrame(empty_slots)
if not df.empty:
    print("\nDetected Empty Slots:")
    print(df)
else:
    print("\nNo empty slots detected.")

"""# **Output 4**"""

# ===============================
# ðŸ“¦ Setup + Upload Everything
# ===============================
uploaded = files.upload()  # Upload all files once (images + .pt model)
uploaded_files = list(uploaded.keys())

image_path = next((f for f in uploaded if f.lower().endswith(('jpg', 'jpeg', 'png'))), None)
model_path = next((f for f in uploaded_files if f.lower().endswith('.pt')), None)
assert model_path, "âŒ YOLO model file (.pt) not found!"

# ===============================
# ðŸ“· 1. YOLO Validation Visuals
# ===============================

model = YOLO(model_path)

# Step 3: Run inference on the uploaded image
results = model(image_path, conf=0.1)

# Step 4: Plot and display the result directly (no saving to disk)
rendered_img = results[0].plot()  # Returns a NumPy RGB image

# Step 5: Display using matplotlib
print("ðŸ” Our Model detection: ")
plt.figure(figsize=(10, 10))
plt.axis('off')
plt.imshow(rendered_img)
plt.show()

# ===============================
# ðŸ§  2. Segment Anything (SAM)
# ===============================
print("ðŸ“¥ Loading SAM model...")
sam = sam_model_registry["vit_h"](checkpoint="/content/models/sam_vit_h_4b8939.pth")
device = "cuda" if torch.cuda.is_available() else "cpu"
sam.to(device)

# Generate masks
mask_generator = SamAutomaticMaskGenerator(sam)
image_bgr = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
masks = mask_generator.generate(image_rgb)

# Visualize the image with masks
def show_masked_image(image, masks):
    masked_image = image.copy()
    for mask in masks:
        m = mask["segmentation"]
        color = np.random.randint(0, 255, (3,), dtype=np.uint8)
        masked_image[m] = masked_image[m] * 0.5 + color * 0.5
    return masked_image

result_image = show_masked_image(image_rgb, masks)

# Plot the result
plt.figure(figsize=(10, 10))
plt.imshow(result_image)
plt.axis("off")
plt.title("Image with SAM Segmentation")
plt.show()

# ===============================
# ðŸš˜ 3. Misparking Detection
# ===============================
# Function to extract bounding boxes from SAM masks
def extract_boxes_from_masks(masks):
    boxes = []
    for mask in masks:
        segmentation = mask["segmentation"]
        y_indices, x_indices = np.where(segmentation)
        if len(x_indices) > 0 and len(y_indices) > 0:
            x_min, x_max = x_indices.min(), x_indices.max()
            y_min, y_max = y_indices.min(), y_indices.max()
            boxes.append([x_min, y_min, x_max, y_max])
    return boxes

# Get bounding boxes for all segmented regions
slot_boxes = extract_boxes_from_masks(masks)

# Function to calculate IoU
def calculate_iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    interArea = max(0, xB - xA) * max(0, yB - yA)
    boxAArea = max(1, (boxA[2] - boxA[0])) * max(1, (boxA[3] - boxA[1]))
    boxBArea = max(1, (boxB[2] - boxB[0])) * max(1, (boxB[3] - boxB[1]))
    return interArea / float(boxAArea + boxBArea - interArea + 1e-5)

# Detect misparked vehicles
misparked_vehicles = []
for i, vbox in enumerate(slot_boxes):
    count_overlap = 0
    for j, sbox in enumerate(slot_boxes):
        if i != j and calculate_iou(vbox, sbox) > 0.3:
            count_overlap += 1
    if count_overlap >= 2:
        misparked_vehicles.append(vbox)

# ðŸ”¢ Count of misparked vehicles
misparked_count = len(misparked_vehicles)
print(f"ðŸš« Number of misparked vehicles detected: {misparked_count}")

# âœ… Draw misparked vehicle boxes
output = image_rgb.copy()
for box in misparked_vehicles:
    cv2.rectangle(output, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)
    cv2.putText(output, "Misparked", (box[0], box[1] - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)

# ðŸ“· Show final image
plt.figure(figsize=(10, 10))
plt.imshow(output)
plt.axis("off")
plt.title(f"Misparked Vehicles: {misparked_count}")
plt.show()

# ===============================
# ðŸ”¥ 4. Explainable Detection + Heatmap
# ===============================
print("ðŸ§  Explainable Detection on uploaded image...")
model = YOLO(model_path)
results = model(image_path, conf=0.1)
heatmap = np.zeros((image_rgb.shape[0], image_rgb.shape[1]), dtype=np.float32)

# Draw detections and build heatmap
for result in results:
    boxes = result.boxes
    for box in boxes:
        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
        conf = box.conf[0].cpu().numpy()
        cls = int(box.cls[0].cpu().numpy())
        label = model.names[cls]

        # Color code based on label
        color = (0, 255, 0) if label == 'space-empty' else (255, 0, 0)
        cv2.rectangle(image_rgb, (x1, y1), (x2, y2), color, 2)
        cv2.putText(image_rgb, f"{label} {conf:.2f}", (x1, y1-5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        # Increase heatmap intensity in the detected region by confidence
        heatmap[y1:y2, x1:x2] += conf

# Normalize heatmap to 0-255
heatmap = np.clip(heatmap / heatmap.max() * 255, 0, 255).astype(np.uint8)

# Apply colormap (JET: blue to red)
colored_heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
colored_heatmap = cv2.cvtColor(colored_heatmap, cv2.COLOR_BGR2RGB)

# Blend heatmap and original image
overlay = cv2.addWeighted(image_rgb, 0.7, colored_heatmap, 0.3, 0)

# Display results
plt.figure(figsize=(18, 8))

plt.subplot(1, 3, 1)
plt.title("YOLO Detections (Explainability)")
plt.imshow(image_rgb)
plt.axis('off')

plt.subplot(1, 3, 2)
plt.title("Detection Confidence Heatmap")
plt.imshow(colored_heatmap)
plt.axis('off')

plt.subplot(1, 3, 3)
plt.title("Overlay: Detection + Heatmap")
plt.imshow(overlay)
plt.axis('off')

plt.tight_layout()
plt.show()

# ===============================
# ðŸ”¥ 5. occupied slots Explainable Detection + Heatmap
# ===============================

# Load YOLO model
model = YOLO(model_path)

# Run detection
results = model(image_path, conf=0.25)
image = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Process detections â€” only occupied slots
occupied_slots = []
for result in results:
    for box, cls, conf in zip(result.boxes.xyxy, result.boxes.cls, result.boxes.conf):
        x1, y1, x2, y2 = map(int, box)
        class_name = model.names[int(cls)]
        if class_name.lower() == 'space-occupied':
            crop = image_rgb[y1:y2, x1:x2]

            # Saliency heatmap (edge-based explanation)
            gray_crop = cv2.cvtColor(crop, cv2.COLOR_RGB2GRAY)
            saliency = cv2.Laplacian(gray_crop, cv2.CV_64F)
            heatmap = np.absolute(saliency)

            # Robust normalization to 0-255 range
            heatmap_norm = cv2.normalize(heatmap, None, 0, 255, cv2.NORM_MINMAX)
            heatmap_uint8 = heatmap_norm.astype(np.uint8)

            # Apply a visually balanced colormap
            heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_TURBO)

            # Blend heatmap with original crop for better explainability
            blended = cv2.addWeighted(crop, 0.6, heatmap_color, 0.4, 0)

            # Explanation based on pixel edge density
            density = np.mean(heatmap_uint8)
            explanation = "High visual complexity â€” likely due to vehicle presence." if density > 20 else \
                          "Distinct edges and shapes â€” consistent with occupied state."

            # Draw result on original image
            cv2.rectangle(image_rgb, (x1, y1), (x2, y2), (255, 0, 0), 2)
            cv2.putText(image_rgb, f'Occupied {conf:.2f}', (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

            # Display visuals
            plt.figure(figsize=(16, 4))
            plt.subplot(1, 3, 1)
            plt.imshow(crop)
            plt.title("Slot Crop")

            plt.subplot(1, 3, 2)
            plt.imshow(blended)
            plt.title("Blended Heatmap")

            plt.subplot(1, 3, 3)
            plt.text(0, 0.5, explanation, fontsize=12, wrap=True)
            plt.axis('off')
            plt.title("Explanation")

            plt.tight_layout()
            plt.show()

            occupied_slots.append({
                'Box': (x1, y1, x2, y2),
                'Confidence': round(conf.item(), 3),
                'Density': round(density, 2),
                'Explanation': explanation
            })

# Show summary
df = pd.DataFrame(occupied_slots)
if not df.empty:
    print("\nDetected Occupied Slots:")
    print(df)
else:
    print("\nNo occupied slots detected.")


# ===============================
# ðŸ”¥ 6. non-occupied slots Explainable Detection + Heatmap
# ===============================

# Load YOLO model
model = YOLO(model_path)

# Run detection
results = model(image_path, conf=0.25)
image = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Process detections â€” get only empty slots
empty_slots = []
for result in results:
    for box, cls, conf in zip(result.boxes.xyxy, result.boxes.cls, result.boxes.conf):
        x1, y1, x2, y2 = map(int, box)
        class_name = model.names[int(cls)]
        if class_name.lower() == 'space-empty':
            crop = image_rgb[y1:y2, x1:x2]

            # Heatmap by saliency (simple for now â€” use Grad-CAM if needed)
            gray_crop = cv2.cvtColor(crop, cv2.COLOR_RGB2GRAY)
            saliency = cv2.Laplacian(gray_crop, cv2.CV_64F)
            heatmap = np.absolute(saliency)
            heatmap = (heatmap / heatmap.max() * 255).astype(np.uint8)
            heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

            # Explanation logic
            density = np.mean(heatmap)
            explanation = "Uniform low-detail region â€” indicates empty space." if density < 20 else "High variance; possible obstruction."

            # Draw result on image
            cv2.rectangle(image_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(image_rgb, f'Empty {conf:.2f}', (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

            # Show visuals
            plt.figure(figsize=(12, 4))
            plt.subplot(1, 3, 1)
            plt.imshow(crop)
            plt.title("Slot Crop")

            plt.subplot(1, 3, 2)
            plt.imshow(heatmap_color)
            plt.title("Heatmap")

            plt.subplot(1, 3, 3)
            plt.text(0, 0.5, explanation, fontsize=12, wrap=True)
            plt.axis('off')
            plt.title("Explanation")

            plt.tight_layout()
            plt.show()

            empty_slots.append({
                'Box': (x1, y1, x2, y2),
                'Confidence': round(conf.item(), 3),
                'Density': round(density, 2),
                'Explanation': explanation
            })

# Summary Table
df = pd.DataFrame(empty_slots)
if not df.empty:
    print("\nDetected Empty Slots:")
    print(df)
else:
    print("\nNo empty slots detected.")